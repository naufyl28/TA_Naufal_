{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e9863c6-b62f-45ed-80cf-1eb67230bbad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\anaconda\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\anaconda\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\anaconda\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\anaconda\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: transformers[torch] in c:\\anaconda\\lib\\site-packages (4.38.0)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\anaconda\\lib\\site-packages (from transformers[torch]) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda\\lib\\site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda\\lib\\site-packages (from transformers[torch]) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\anaconda\\lib\\site-packages (from transformers[torch]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\anaconda\\lib\\site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\anaconda\\lib\\site-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: torch in c:\\anaconda\\lib\\site-packages (from transformers[torch]) (2.7.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\anaconda\\lib\\site-packages (from transformers[torch]) (0.26.0)\n",
      "Requirement already satisfied: psutil in c:\\anaconda\\lib\\site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\anaconda\\lib\\site-packages (from torch->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\anaconda\\lib\\site-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda\\lib\\site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (from torch->transformers[torch]) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests->transformers[torch]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests->transformers[torch]) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: accelerate==0.26.0 in c:\\anaconda\\lib\\site-packages (0.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda\\lib\\site-packages (from accelerate==0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda\\lib\\site-packages (from accelerate==0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\anaconda\\lib\\site-packages (from accelerate==0.26.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\anaconda\\lib\\site-packages (from accelerate==0.26.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\anaconda\\lib\\site-packages (from accelerate==0.26.0) (2.7.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\anaconda\\lib\\site-packages (from accelerate==0.26.0) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\anaconda\\lib\\site-packages (from accelerate==0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\anaconda\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\anaconda\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\anaconda\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (75.1.0)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (from huggingface-hub->accelerate==0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\anaconda\\lib\\site-packages (from huggingface-hub->accelerate==0.26.0) (4.66.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.26.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate==0.26.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2025.4.26)\n",
      "Requirement already satisfied: emoji in c:\\anaconda\\lib\\site-packages (2.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers[torch]\n",
    "!pip install accelerate==0.26.0\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ae3f1c-8514-4052-a4de-adb1a6f23f8a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0506 19:05:10.778000 5328 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae9a38-cb2c-4f20-a5e8-556d531795ae",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21715b59-b960-4fa5-bbc0-5d74e2dbb6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>label_manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hutan Bojonegoro udah habis di tebang i,,dan d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jangan khawatir deforestasi!!, kata pak presiden</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assalamu&amp;#39;alaikum, izin mengambil salah sat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saya tetap yakin kita terlahir dari nenek moya...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sekarang rasakan panasnya nggk seperti biasany...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sbnr nya hutan milik siapa? Kok berani membaba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>demi menyenangkan sahabat jokowi, si haji Is*m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mampus.....Suatu saat kalimantan bakalan gundul</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kalau orang hidup di kabupaten KKU berdampinga...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kalimantan rusak itu akibat ulah pejabat dan o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         textDisplay  label_manual\n",
       "0  Hutan Bojonegoro udah habis di tebang i,,dan d...             0\n",
       "1   Jangan khawatir deforestasi!!, kata pak presiden             1\n",
       "2  Assalamu&#39;alaikum, izin mengambil salah sat...             1\n",
       "3  Saya tetap yakin kita terlahir dari nenek moya...             2\n",
       "4  sekarang rasakan panasnya nggk seperti biasany...             1\n",
       "5  Sbnr nya hutan milik siapa? Kok berani membaba...             1\n",
       "6  demi menyenangkan sahabat jokowi, si haji Is*m...             0\n",
       "7    Mampus.....Suatu saat kalimantan bakalan gundul             0\n",
       "8  Kalau orang hidup di kabupaten KKU berdampinga...             0\n",
       "9  Kalimantan rusak itu akibat ulah pejabat dan o...             0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/ASUS/Desktop/Tugasakhir_M Naufal/Dataset/Data TA_LABEL.csv')\n",
    "df['label_manual'] = df['label_manual'].fillna(0).astype(int)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b41d926c-2fde-4015-a140-38754ba664b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(857, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bb9dba7-3ff4-431c-aa06-d625a497845a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  label_manual\n",
      "0  Hutan Bojonegoro udah habis di tebang idan di ...             0\n",
      "1      Jangan khawatir deforestasi kata pak presiden             1\n",
      "2  Assalamualaikum izin mengambil salah satu klip...             1\n",
      "3  Saya tetap yakin kita terlahir dari nenek moya...             2\n",
      "4  sekarang rasakan panasnya nggk seperti biasany...             1\n",
      "5  Sbnr nya hutan milik siapa Kok berani membabal...             1\n",
      "6  demi menyenangkan sahabat jokowi si haji Ism j...             0\n",
      "7         MampusSuatu saat kalimantan bakalan gundul             0\n",
      "8  Kalau orang hidup di kabupaten KKU berdampinga...             0\n",
      "9  Kalimantan rusak itu akibat ulah pejabat dan o...             0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        if re.search(r'<.*?>', text):\n",
    "            text = BeautifulSoup(text, \"html.parser\").get_text() \n",
    "\n",
    "        text = emoji.replace_emoji(text, replace='')  \n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)  \n",
    "        text = re.sub(r'\\s+', ' ', text)  \n",
    "        text = text.strip()  \n",
    "    else:\n",
    "        text = \"\"  \n",
    "    \n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['textDisplay'].apply(clean_text)\n",
    "\n",
    "df = df[df['cleaned_text'] != \"\"]\n",
    "\n",
    "print(df[['cleaned_text', 'label_manual']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef1f0dd-4be2-423d-94ea-29d73af1672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'C:/Users/ASUS/Desktop/Tugasakhir_M Naufal/Dataset/dataset_preprocessing.csv'\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1980566a-aa28-4b20-8fcc-f29a14a845b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  \\\n",
      "0  Hutan Bojonegoro udah habis di tebang idan di ...   \n",
      "1      Jangan khawatir deforestasi kata pak presiden   \n",
      "2  Assalamualaikum izin mengambil salah satu klip...   \n",
      "3  Saya tetap yakin kita terlahir dari nenek moya...   \n",
      "4  sekarang rasakan panasnya nggk seperti biasany...   \n",
      "5  Sbnr nya hutan milik siapa Kok berani membabal...   \n",
      "6  demi menyenangkan sahabat jokowi si haji Ism j...   \n",
      "7         MampusSuatu saat kalimantan bakalan gundul   \n",
      "8  Kalau orang hidup di kabupaten KKU berdampinga...   \n",
      "9  Kalimantan rusak itu akibat ulah pejabat dan o...   \n",
      "\n",
      "                                     normalized_text  \n",
      "0  hutan bojonegoro udah habis di tebang dan di t...  \n",
      "1      jangan khawatir deforestasi kata pak presiden  \n",
      "2  assalamualaikum izin mengambil salah satu klip...  \n",
      "3  saya tetap yakin kita terlahir dari nenek moya...  \n",
      "4  sekarang rasakan panasnya nggk seperti biasany...  \n",
      "5  sebenarnya  hutan milik siapa  berani membabal...  \n",
      "6  demi menyenangkan sahabat jokowi si haji ism j...  \n",
      "7         mampussuatu saat kalimantan bakalan gundul  \n",
      "8  kalau orang hidup di kabupaten kku berdampinga...  \n",
      "9  kalimantan rusak itu akibat ulah pejabat dan o...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/ASUS/Desktop/Tugasakhir_M Naufal/Dataset/dataset_preprocessing.csv')\n",
    "\n",
    "\n",
    "normalization_dict = {\n",
    "    'sbnrnya': 'sebenarnya',\n",
    "    'sbnr': 'sebenarnya',\n",
    "    'ak': 'aku',\n",
    "    'gk': 'gak',\n",
    "    'gak': 'tidak',\n",
    "    'yg': 'yang',\n",
    "    'klo': 'kalau',\n",
    "    'kl': 'kalau',\n",
    "    'bg': 'banget',\n",
    "    'gt': 'gitu',\n",
    "    'gtu': 'gitu',\n",
    "    'jgn': 'jangan',\n",
    "    'gpp': 'gapapa',\n",
    "    'jd': 'jadi',\n",
    "    'org': 'orang',\n",
    "    'tak': 'tidak',\n",
    "    'tgung': 'tanggung',\n",
    "    'sdh': 'sudah',\n",
    "    'dah': 'sudah',\n",
    "    'udh': 'sudah',\n",
    "    'dh': 'sudah',\n",
    "    'bgt': 'banget',\n",
    "    'tmn': 'teman',\n",
    "    'br': 'baru',\n",
    "    'lbh': 'lebih',\n",
    "    'krn': 'karena',\n",
    "    'SDA': 'sumber daya alam',\n",
    "    'seblm': 'sebelum',\n",
    "    'skrg': 'sekarang',\n",
    "    'skr': 'sekarang',\n",
    "    'gedhe': 'gede',\n",
    "    'ngk': 'engga',\n",
    "    'kn': 'kan',\n",
    "    'byk': 'banyak',\n",
    "    'bnyk': 'banyak',\n",
    "    'gw': 'aku',\n",
    "    'gue': 'aku',\n",
    "    'gwk': 'aku',\n",
    "    'loe': 'kamu',\n",
    "    'lu': 'kamu',\n",
    "    'lo': 'kamu',\n",
    "    'elu': 'kamu',\n",
    "    'elo': 'kamu',\n",
    "    'napa': 'kenapa',\n",
    "    'knp': 'kenapa',\n",
    "    'mgkn': 'mungkin',\n",
    "    'gmn': 'gimana',\n",
    "    'bkn': 'bukan',\n",
    "    'bs': 'bisa',\n",
    "    'bisa2': 'bisa-bisa',\n",
    "    'diaa': 'dia',\n",
    "    'bgtu': 'begitu',\n",
    "    'btw': 'ngomong-ngomong',\n",
    "    'dongg': 'dong',\n",
    "    'gituu': 'gitu',\n",
    "    'ntr': 'nanti',\n",
    "    'msh': 'masih',\n",
    "    'mauu': 'mau',\n",
    "    'yakinbgt': 'yakin banget',\n",
    "    'sampe': 'sampai',\n",
    "    'mo': 'mau',\n",
    "    'kek': 'seperti',\n",
    "    'kyk': 'seperti',\n",
    "    'ky': 'kayak',\n",
    "    'ga': 'tidak',\n",
    "    'enggak': 'tidak',\n",
    "    'engga': 'tidak',\n",
    "    'nggak': 'tidak',\n",
    "    'ngga': 'tidak',\n",
    "    'tdk': 'tidak',\n",
    "    'okee': 'oke',\n",
    "    'okeeee': 'oke',\n",
    "    'gtw': 'gatau',\n",
    "    'gatau': 'tidak tahu',\n",
    "    'gaada': 'tidak ada',\n",
    "    'gakada': 'tidak ada',\n",
    "    'gbs': 'tidak bisa',\n",
    "    'gabisa': 'tidak bisa',\n",
    "    'gajadi': 'tidak jadi',\n",
    "    'gapapa': 'tidak apa-apa',\n",
    "    'gausah': 'tidak usah',\n",
    "    'udahlah': 'sudah lah',\n",
    "    'kmn': 'kemana',\n",
    "    'kmna': 'kemana',\n",
    "    'sma': 'sama',\n",
    "    'mls': 'malas',\n",
    "    'mlas': 'malas',\n",
    "    'trs2': 'terus-terus',\n",
    "    'trus2': 'terus-terus',\n",
    "    'mager': 'malas gerak',\n",
    "    'drpd': 'daripada',\n",
    "    'abis': 'habis',\n",
    "    'mg': 'minggu',\n",
    "    'biar': 'agar',\n",
    "    'ampun': 'tidak sanggup',\n",
    "    'parahh': 'parah',\n",
    "    'zz': '',\n",
    "    'zzz': '',\n",
    "    'wkwk': '',\n",
    "    'wkwkwk': '',\n",
    "    'lol': '',\n",
    "    'hehe': '',\n",
    "    'haha': '',\n",
    "    'huhu': '',\n",
    "    'yaa': 'iya',\n",
    "    'iyaaa': 'iya',\n",
    "    'iiya': 'iya',\n",
    "    'okei': 'oke',\n",
    "    'okehh': 'oke',\n",
    "    'heemm': '',\n",
    "    'tp': 'tapi',\n",
    "    'dmn': 'dimana',\n",
    "    'aja': 'saja',\n",
    "    'dg': 'dengan',\n",
    "    'dr': 'dari',\n",
    "    'dpt': 'dapat',\n",
    "    'karna': 'karena',\n",
    "    'blm': 'belum',\n",
    "    'skrng': 'sekarang',\n",
    "    'ntar': 'nanti',\n",
    "    'kpn': 'kapan',\n",
    "    'trs': 'terus',\n",
    "    'pdhl': 'padahal',\n",
    "    'kmrn': 'kemarin',\n",
    "    'km': 'kamu',\n",
    "    'lg': 'lagi',\n",
    "    'mknya': 'makanya',\n",
    "    'trus': 'terus',\n",
    "    'donk': 'dong',\n",
    "    'sih': '',\n",
    "    'deh': '',\n",
    "    'nya': '',\n",
    "    'kok': '',\n",
    "    'hdh': '',\n",
    "    'cuma': 'hanya',\n",
    "    'cuman': 'hanya',\n",
    "    'y': 'ya',\n",
    "    'thx': 'terima kasih',\n",
    "    'thnk': 'terima kasih',\n",
    "    'makasih': 'terima kasih',\n",
    "    'thanks': 'terima kasih',\n",
    "    'sm': 'sama',\n",
    "    'sama2': 'sama-sama',\n",
    "    'sama2': 'sama-sama',\n",
    "    'idan': 'dan',\n",
    "    'pd': 'pada',\n",
    "    'ya': 'iya',\n",
    "    'nggk ': 'engga'\n",
    "}\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()  \n",
    "    words = text.split()  \n",
    "    normalized_words = [normalization_dict.get(word, word) for word in words]  \n",
    "    return ' '.join(normalized_words)  \n",
    "\n",
    "df['normalized_text'] = df['cleaned_text'].apply(normalize_text)\n",
    "print(df[['cleaned_text', 'normalized_text']].head(10))\n",
    "\n",
    "output_path = 'C:/Users/ASUS/Desktop/Tugasakhir_M Naufal/Dataset/cleaned_text.csv'\n",
    "df[['normalized_text', 'label_manual']].to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc5a56-d901-46fa-842c-5f80b1ccf346",
   "metadata": {},
   "source": [
    "Tokenisasi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fbdd9a6-8512-47e1-803e-57048b9497d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tokens  \\\n",
      "0  [[CLS], hutan, bojonegoro, udah, habis, di, te...   \n",
      "1  [[CLS], jangan, khawatir, def, ##ores, ##tasi,...   \n",
      "2  [[CLS], assalamualaikum, izin, mengambil, sala...   \n",
      "3  [[CLS], saya, tetap, yakin, kita, terlahir, da...   \n",
      "4  [[CLS], sekarang, rasakan, panasnya, ng, ##g, ...   \n",
      "5  [[CLS], sebenarnya, hutan, milik, siapa, beran...   \n",
      "6  [[CLS], demi, menyenangkan, sahabat, jokowi, s...   \n",
      "7  [[CLS], mampu, ##ss, ##uatu, saat, kalimantan,...   \n",
      "8  [[CLS], kalau, orang, hidup, di, kabupaten, kk...   \n",
      "9  [[CLS], kalimantan, rusak, itu, akibat, ulah, ...   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0  [3, 3283, 15125, 9988, 5594, 1485, 5491, 1480,...  \n",
      "1  [3, 3132, 6348, 4747, 18077, 4823, 1951, 2458,...  \n",
      "2  [3, 26460, 5297, 2948, 1911, 1713, 13777, 1485...  \n",
      "3  [3, 1731, 2261, 4455, 1732, 14368, 1542, 6551,...  \n",
      "4  [3, 2338, 14220, 26061, 2870, 938, 942, 1730, ...  \n",
      "5  [3, 2923, 3283, 3139, 3476, 5115, 1672, 7617, ...  \n",
      "6  [3, 3887, 8536, 4887, 27930, 1764, 5075, 7363,...  \n",
      "7  [3, 2528, 13292, 28624, 1759, 4092, 26575, 242...  \n",
      "8  [3, 2280, 1646, 2025, 1485, 2271, 6015, 943, 1...  \n",
      "9  [3, 4092, 5236, 1570, 2468, 11964, 3788, 1501,...  \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "\n",
    "#  tokenizer untuk IndoBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:/Users/ASUS/Desktop/Tugasakhir_M Naufal/Dataset/cleaned_text.csv')\n",
    "\n",
    "# tokenisasi, padding, dan truncation\n",
    "def tokenize_text(text):\n",
    "    if isinstance(text, str) and text.strip() != \"\":\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  \n",
    "            max_length=128,           \n",
    "            padding='max_length',     \n",
    "            truncation=True,          \n",
    "            return_tensors='pt'       \n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten().tolist()  \n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)  \n",
    "        return tokens, input_ids\n",
    "    else:\n",
    "        return [], [] \n",
    "\n",
    "df['tokens'] = df['normalized_text'].apply(lambda x: tokenize_text(x)[0])\n",
    "df['tokenized_text'] = df['normalized_text'].apply(lambda x: tokenize_text(x)[1])\n",
    "\n",
    "print(df[['tokens', 'tokenized_text']].head(10))\n",
    "\n",
    "output_path = 'C:/Users/ASUS/Desktop/Tugasakhir_M Naufal/Dataset/dataset_tokenized_indobert.csv'\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add5b8d-6857-450d-9499-cc110ea64e97",
   "metadata": {},
   "source": [
    "Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4845a39c-b9a9-4fb2-8190-c6288928d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask(input_ids):\n",
    "    attention_mask = [1 if token_id != tokenizer.pad_token_id else 0 for token_id in input_ids]\n",
    "    return attention_mask\n",
    "\n",
    "df['attention_mask'] = df['tokenized_text'].apply(lambda x: create_attention_mask(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799cc657-5558-4ee2-92e5-79cf5620d40d",
   "metadata": {},
   "source": [
    "Konversi ke Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed114a9d-4820-4e8f-ba76-0d0530b6a3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def convert_to_tensor(input_ids, attention_mask):\n",
    "    input_ids_tensor = torch.tensor(input_ids)\n",
    "    attention_mask_tensor = torch.tensor(attention_mask)\n",
    "    return input_ids_tensor, attention_mask_tensor\n",
    "\n",
    "df['input_ids_tensor'], df['attention_mask_tensor'] = zip(*df.apply(lambda row: convert_to_tensor(row['tokenized_text'], row['attention_mask']), axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7bcbe0a-8bf8-4a03-a77f-bf48bba92939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data untuk training set: 609\n",
      "Jumlah data untuk validation set: 68\n",
      "Jumlah data untuk test set: 170\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#  data menjadi train dan test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "#  data train menjadi train dan validation\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"Jumlah data untuk training set: {len(train_df)}\")\n",
    "print(f\"Jumlah data untuk validation set: {len(val_df)}\")\n",
    "print(f\"Jumlah data untuk test set: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f606c80-128c-4d43-a1d9-ba2712c7dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Menghitung bobot kelas berdasarkan distribusi label\n",
    "class_weights = [1 / df['label_manual'].value_counts().get(i, 1) for i in range(3)]\n",
    "\n",
    "# Konversi ke tensor\n",
    "class_weights_tensor = torch.tensor(class_weights)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7989932-318a-4d40-839e-2a68e7679a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = SentimentDataset(\n",
    "    torch.stack(train_df['input_ids_tensor'].tolist()),\n",
    "    torch.stack(train_df['attention_mask_tensor'].tolist()),\n",
    "    torch.tensor(train_df['label_manual'].values)\n",
    ")\n",
    "\n",
    "val_dataset = SentimentDataset(\n",
    "    torch.stack(val_df['input_ids_tensor'].tolist()),\n",
    "    torch.stack(val_df['attention_mask_tensor'].tolist()),\n",
    "    torch.tensor(val_df['label_manual'].values)\n",
    ")\n",
    "\n",
    "test_dataset = SentimentDataset(\n",
    "    torch.stack(test_df['input_ids_tensor'].tolist()),\n",
    "    torch.stack(test_df['attention_mask_tensor'].tolist()),\n",
    "    torch.tensor(test_df['label_manual'].values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b5889c3-51cc-40fb-b6c5-696443e8b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom BERT classifier with an optional hidden layer\n",
    "class CustomBertClassifier(nn.Module):\n",
    "    def __init__(self, base_model, num_labels=3, hidden_layer=False):\n",
    "        super().__init__()\n",
    "        self.bert = base_model\n",
    "        self.num_labels = num_labels\n",
    "        if hidden_layer:\n",
    "            self.hidden = nn.Linear(self.bert.config.hidden_size, 128)\n",
    "            self.classifier = nn.Linear(128, num_labels)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        if hasattr(self, 'hidden'):\n",
    "            hidden_state = self.hidden(hidden_state)\n",
    "        logits = self.classifier(hidden_state)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0341c29-14e8-4735-9ab6-c05134960063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "use_hidden_layer = True\n",
    "model = CustomBertClassifier(base_model=AutoModel.from_pretrained(\"indolem/indobert-base-uncased\"), hidden_layer=use_hidden_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b336bcab-39ad-462a-9f89-55f05df6128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing the BERT backbone layers\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Only the classifier layer will be updated\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79297c2f-4c2d-4610-abcf-b9c271bbe62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"indolem/indobert-base-uncased\", \n",
    "    num_labels=3  \n",
    ")\n",
    "\n",
    "# Optimizer dan scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31d3e085-1c65-4e30-9324-7daf93b41fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV logging function\n",
    "def log_metrics_to_csv(metrics, epoch, filename='metrics_log.csv'):\n",
    "    file_exists = False\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            file_exists = True\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['epoch', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "\n",
    "        writer.writerow([epoch, metrics['accuracy'], metrics['precision'], metrics['recall'], metrics['f1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6a5b9bd-3eac-4cdd-9b79-5348d7633583",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=1)  \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'precision': precision_score(labels, preds, average='weighted'),\n",
    "        'recall': recall_score(labels, preds, average='weighted'),\n",
    "        'f1': f1_score(labels, preds, average='weighted')\n",
    "    }\n",
    "\n",
    "    log_metrics_to_csv(metrics, trainer.state.epoch)  \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "642572bd-b7c9-4c50-a0dd-1220c7207db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rata-rata waktu komputasi untuk satu data point: 0.1711 detik\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def measure_inference_time(model, data_point, num_runs=10):\n",
    "    total_time = 0\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            input_ids = data_point['input_ids'].unsqueeze(0)  \n",
    "            attention_mask = data_point['attention_mask'].unsqueeze(0)  \n",
    "            _ = model(input_ids, attention_mask=attention_mask)\n",
    "        total_time += time.time() - start_time\n",
    "    average_time = total_time / num_runs\n",
    "    return average_time\n",
    "\n",
    "sample_data_point = val_dataset[0]  \n",
    "avg_inference_time = measure_inference_time(model, sample_data_point)\n",
    "print(f\"Rata-rata waktu komputasi untuk satu data point: {avg_inference_time:.4f} detik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7f9e170-0215-484e-926e-48039d4b87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',           \n",
    "    num_train_epochs=20,              \n",
    "    per_device_train_batch_size=16,   \n",
    "    per_device_eval_batch_size=64,    \n",
    "    warmup_steps=500,                 \n",
    "    weight_decay=0.01,                \n",
    "    logging_dir='./logs',             \n",
    "    logging_steps=1,                  \n",
    "    evaluation_strategy=\"epoch\",      \n",
    "    save_strategy=\"epoch\",            \n",
    "    load_best_model_at_end=True,      \n",
    "    lr_scheduler_type=\"linear\",       \n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset,            \n",
    "    compute_metrics=compute_metrics,     \n",
    "    optimizers=(optimizer, None),        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9067dfa9-6123-457d-add7-74e052b86019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [780/780 47:45, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.220500</td>\n",
       "      <td>1.084343</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.532428</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.344254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.841300</td>\n",
       "      <td>1.078685</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.512374</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.351884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.132400</td>\n",
       "      <td>1.069519</td>\n",
       "      <td>0.338235</td>\n",
       "      <td>0.461172</td>\n",
       "      <td>0.338235</td>\n",
       "      <td>0.338817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.074700</td>\n",
       "      <td>1.057336</td>\n",
       "      <td>0.338235</td>\n",
       "      <td>0.452378</td>\n",
       "      <td>0.338235</td>\n",
       "      <td>0.346081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.057000</td>\n",
       "      <td>1.042272</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.442857</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.400735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.060300</td>\n",
       "      <td>1.025155</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.468665</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.459913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.084400</td>\n",
       "      <td>1.006886</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.451184</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.452705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.335800</td>\n",
       "      <td>0.987825</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444118</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.468173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.889500</td>\n",
       "      <td>0.968816</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.466243</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.498102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.949200</td>\n",
       "      <td>0.949799</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.442191</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.483922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.047200</td>\n",
       "      <td>0.933991</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.459559</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.490659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.666700</td>\n",
       "      <td>0.918656</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.459559</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.490659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.904600</td>\n",
       "      <td>0.905021</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.530749</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.495692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.679800</td>\n",
       "      <td>0.896498</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.530749</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.495692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.675200</td>\n",
       "      <td>0.890685</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.530749</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.495692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.709700</td>\n",
       "      <td>0.886530</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.530749</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.495692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.782100</td>\n",
       "      <td>0.884056</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.530749</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.495692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.747700</td>\n",
       "      <td>0.882250</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.381488</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.471658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.766100</td>\n",
       "      <td>0.881198</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.381488</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.471658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.470200</td>\n",
       "      <td>0.880893</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.381488</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.471658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 03:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./final_model\\\\tokenizer_config.json',\n",
       " './final_model\\\\special_tokens_map.json',\n",
       " './final_model\\\\vocab.txt',\n",
       " './final_model\\\\added_tokens.json',\n",
       " './final_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate(test_dataset)\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model('./final_model')\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained('./final_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
